{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b567863d-3f33-42a7-b5b0-f8277972974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "import wandb\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afca5762-af09-48de-9bed-7460c952e8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed20f19-0f1b-4516-b2d0-d996c9de67a2",
   "metadata": {},
   "source": [
    "### Hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3deedff9-8418-4172-bbf8-e21937d0e409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'seed':42,\n",
    "    'exp_name': 'Test',\n",
    "    'torch_deterministic':True,\n",
    "    'cuda':False,\n",
    "    'track': True,\n",
    "    'wandb_project_name': 'Test',\n",
    "    'wandb_entity': None,\n",
    "    'capture_video': False,\n",
    "    'env_id': \"CartPole-v1\",\n",
    "    'total_timesteps':int(5e3),\n",
    "    'learning_rate': 2.5e-4,\n",
    "    'num_envs' : 4,\n",
    "    'num_steps': 128,\n",
    "    'anneal_lr': True,\n",
    "    'gamma': 0.99,\n",
    "    'gae_lambda': 0.95,\n",
    "    'num_minibatches': 4,\n",
    "    'update_epochs': 4,\n",
    "    'norm_adv': True,\n",
    "    'clip_coef': 0.2,\n",
    "    'clip_vloss': True,\n",
    "    'ent_coef': 0.01,\n",
    "    'vf_coef': 0.5,\n",
    "    'max_grad_norm': 0.5,\n",
    "    'target_kl': None\n",
    "    \n",
    "}\n",
    "args = dotdict(args)\n",
    "\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "\n",
    "print(args.minibatch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abc5540-7ccc-4b72-a756-f9f8419b4f3b",
   "metadata": {},
   "source": [
    "### Sweep config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57d108bf-dca7-4132-961c-e01210663e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'early_terminate':{\n",
    "        'type':'hyperband',\n",
    "        'eta':2,\n",
    "        'min_iter':90,\n",
    "        's':3\n",
    "    }\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'charts/episodic_return',\n",
    "    'goal': 'maximize',\n",
    "    'target':40\n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff27d350-70bf-47bb-a51e-a497c69cc675",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict = {\n",
    "    'ent_coef': {\n",
    "        'values': [0.00001, 0.0001, 0.001, 0.01]\n",
    "    },\n",
    "    # 'policy_layer_size': {\n",
    "    #     'values': [64, 128, 256]\n",
    "    # },\n",
    "    # 'value_layer_size': {\n",
    "    #     'values': [64, 128, 256]\n",
    "    # },\n",
    "    'vf_coef':{\n",
    "        'values':[0.25,0.5,0.75]\n",
    "    },\n",
    "    'n_epochs':{\n",
    "        'values':[5,10,15]\n",
    "    }\n",
    "}\n",
    "parameters_dict.update({\n",
    "    'learning_rate': {\n",
    "        # a flat distribution between 0 and 0.1\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0,\n",
    "        'max': 0.1\n",
    "    },\n",
    "    'batch_size': {\n",
    "        'distribution': 'q_log_uniform_values',\n",
    "        'q': 8,\n",
    "        'min': 100,\n",
    "        'max': 512\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c274372c-00bf-4b1d-8150-07c2df68a7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config['parameters']=parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fe2eb6f-4a96-4c08-b2c3-78ca3c9aad02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'random',\n",
       " 'early_terminate': {'type': 'hyperband', 'eta': 2, 'min_iter': 90, 's': 3},\n",
       " 'metric': {'name': 'charts/episodic_return',\n",
       "  'goal': 'maximize',\n",
       "  'target': 40},\n",
       " 'parameters': {'ent_coef': {'values': [1e-05, 0.0001, 0.001, 0.01]},\n",
       "  'vf_coef': {'values': [0.25, 0.5, 0.75]},\n",
       "  'n_epochs': {'values': [5, 10, 15]},\n",
       "  'learning_rate': {'distribution': 'uniform', 'min': 0, 'max': 0.1},\n",
       "  'batch_size': {'distribution': 'q_log_uniform_values',\n",
       "   'q': 8,\n",
       "   'min': 100,\n",
       "   'max': 512}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sweep_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7941404-f780-4caf-ac0c-cb3d795cf226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: y7yt9bgv\n",
      "Sweep URL: https://wandb.ai/donalexs12/test/sweeps/y7yt9bgv\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb7034f8-5469-4abd-8b52-d2f7b59b7382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id)\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        # env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08c254ca-8ae5-4fbb-bbc7-e67efb2c048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb1b82ba-84ad-46db-8747-17ed3bc96cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n",
    "        )\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffa5d0d7-0136-4e30-aae6-0ea5e12e92a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c927170-855a-405b-a8df-2ef29a25163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if args.track:\n",
    "#     import wandb\n",
    "\n",
    "#     wandb.init(\n",
    "#         project=args.wandb_project_name,\n",
    "#         entity=args.wandb_entity,\n",
    "#         sync_tensorboard=True,\n",
    "#         config=vars(args),\n",
    "#         name=run_name,\n",
    "#         monitor_gym=True,\n",
    "#         save_code=True,\n",
    "#     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cf0d78-c9f8-4736-80a8-d680382aeb6e",
   "metadata": {},
   "source": [
    "### Env creation and seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "556fc225-9af6-48a6-9761-8f48143ffc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "035eadc3-a60f-4a48-9eb2-59d1413d1b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(args.env_id, idx=i, capture_video=args.capture_video, run_name=run_name, seed=args.seed) for i in range(args.num_envs)]\n",
    ")\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26555867-308a-41ab-94a5-9fa3098661a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(envs).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d45d552-9577-4c9d-8b67-83246e5002a9",
   "metadata": {},
   "source": [
    "### Storage setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d20192e-aeea-4848-8425-db57ec58938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "values = torch.zeros((args.num_steps, args.num_envs)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ec854f-5e26-49f7-9541-f2143b839344",
   "metadata": {},
   "source": [
    "### Game start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82227cbd-08ba-480d-9eee-6a9abbb787f4",
   "metadata": {},
   "source": [
    ">ПОПРОБОВАТЬ ВЫНЕСТИ КОНФИГУРИРУЕМЫЕ СВИПОМ ПАРАМЕТРЫ ЗА ПРЕДЕЛЫ ЦИКЛА"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb0b4a6-7afa-4412-8e05-4c9b2d0b051a",
   "metadata": {},
   "source": [
    "### Playing cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bbc6b16-f8b4-4cd4-bf5e-019882c2d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_PPO(config=None):\n",
    "    with wandb.init(config=config,\n",
    "                        sync_tensorboard=True,\n",
    "                        project='SFQ_hyperparam'):\n",
    "\n",
    "        writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "        writer.add_text(\n",
    "            \"hyperparameters\",\n",
    "            \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "        )\n",
    "        \n",
    "        config = wandb.config\n",
    "        \n",
    "        args.batch_size = config.batch_size\n",
    "        args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "        print(args.batch_size)\n",
    "        global_step = 0\n",
    "        start_time = time.time()\n",
    "        next_obs = torch.Tensor(envs.reset()[0]).to(device)\n",
    "        next_done = torch.zeros(args.num_envs).to(device)\n",
    "        num_updates = args.total_timesteps // args.batch_size\n",
    "        optimizer = optim.Adam(agent.parameters(), lr=config.learning_rate, eps=1e-5)\n",
    "        \n",
    "        for update in range(1, num_updates + 1):\n",
    "            # Annealing the rate if instructed to do so.\n",
    "            if args.anneal_lr:\n",
    "                frac = 1.0 - (update - 1.0) / num_updates\n",
    "                lrnow = frac * args.learning_rate\n",
    "                optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "        \n",
    "            for step in range(0, args.num_steps):\n",
    "                global_step += 1 * args.num_envs\n",
    "                obs[step] = torch.Tensor(next_obs).to(device)\n",
    "                dones[step] = next_done\n",
    "        \n",
    "                # ALGO LOGIC: action logic\n",
    "                with torch.no_grad():\n",
    "                    action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "                    values[step] = value.flatten()\n",
    "                actions[step] = action\n",
    "                logprobs[step] = logprob\n",
    "        \n",
    "                # TRY NOT TO MODIFY: execute the game and log data.\n",
    "                next_obs, reward, done,terminated, info = envs.step(action.cpu().numpy())\n",
    "                rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "                next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "        \n",
    "                if 'final_info' in info.keys():\n",
    "                    for item in info['final_info']:\n",
    "                        if item is not None:\n",
    "                            print(f\"global_step={global_step}, episodic_return={item['episode']['r']}, episode_length = {item['episode']['l']}\")\n",
    "                            writer.add_scalar(\"charts/episodic_return\", item[\"episode\"][\"r\"], global_step)\n",
    "                            writer.add_scalar(\"charts/episodic_length\", item[\"episode\"][\"l\"], global_step)\n",
    "                            break\n",
    "               \n",
    "                # bootstrap value if not done\n",
    "                with torch.no_grad():\n",
    "                    next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "                    advantages = torch.zeros_like(rewards).to(device)\n",
    "                    lastgaelam = 0\n",
    "                    for t in reversed(range(args.num_steps)):\n",
    "                        if t == args.num_steps - 1:\n",
    "                            nextnonterminal = 1.0 - next_done\n",
    "                            nextvalues = next_value\n",
    "                        else:\n",
    "                            nextnonterminal = 1.0 - dones[t + 1]\n",
    "                            nextvalues = values[t + 1]\n",
    "                        delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "                        advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "                    returns = advantages + values\n",
    "        \n",
    "                # flatten the batch\n",
    "                b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "                b_logprobs = logprobs.reshape(-1)\n",
    "                b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "                b_advantages = advantages.reshape(-1)\n",
    "                b_returns = returns.reshape(-1)\n",
    "                b_values = values.reshape(-1)\n",
    "        \n",
    "                # Optimizing the policy and value network\n",
    "                b_inds = np.arange(args.batch_size)\n",
    "                clipfracs = []\n",
    "                for epoch in range(args.update_epochs):\n",
    "                    np.random.shuffle(b_inds)\n",
    "                    for start in range(0, args.batch_size, args.minibatch_size):\n",
    "                        end = start + args.minibatch_size\n",
    "                        mb_inds = b_inds[start:end]\n",
    "        \n",
    "                        _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "                        logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                        ratio = logratio.exp()\n",
    "        \n",
    "                        with torch.no_grad():\n",
    "                            # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                            old_approx_kl = (-logratio).mean()\n",
    "                            approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                            clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "        \n",
    "                        mb_advantages = b_advantages[mb_inds]\n",
    "                        if args.norm_adv:\n",
    "                            mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "        \n",
    "                        # Policy loss\n",
    "                        pg_loss1 = -mb_advantages * ratio\n",
    "                        pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "                        pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "        \n",
    "                        # Value loss\n",
    "                        newvalue = newvalue.view(-1)\n",
    "                        if args.clip_vloss:\n",
    "                            v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                            v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                                newvalue - b_values[mb_inds],\n",
    "                                -args.clip_coef,\n",
    "                                args.clip_coef,\n",
    "                            )\n",
    "                            v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                            v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                            v_loss = 0.5 * v_loss_max.mean()\n",
    "                        else:\n",
    "                            v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "        \n",
    "                        entropy_loss = entropy.mean()\n",
    "                        loss = pg_loss - config.ent_coef * entropy_loss + v_loss * config.vf_coef\n",
    "        \n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "                        optimizer.step()\n",
    "        \n",
    "                    if args.target_kl is not None:\n",
    "                        if approx_kl > args.target_kl:\n",
    "                            break\n",
    "        \n",
    "                y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "                var_y = np.var(y_true)\n",
    "                explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "        \n",
    "                # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "                writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "                writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "                writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "                writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "                writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "                writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "                writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "                writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "                #print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "                writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "    envs.close()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89e88c6e-50bc-4a9a-95dd-7cc8a0bffada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gc1omplg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 264\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.07505805831385604\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tvf_coef: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdonalexs12\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\user\\PycharmProjects\\Educational\\wandb\\run-20231107_235504-gc1omplg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/donalexs12/test/runs/gc1omplg' target=\"_blank\">brisk-sweep-1</a></strong> to <a href='https://wandb.ai/donalexs12/test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/donalexs12/test/sweeps/y7yt9bgv' target=\"_blank\">https://wandb.ai/donalexs12/test/sweeps/y7yt9bgv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/donalexs12/test' target=\"_blank\">https://wandb.ai/donalexs12/test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/donalexs12/test/sweeps/y7yt9bgv' target=\"_blank\">https://wandb.ai/donalexs12/test/sweeps/y7yt9bgv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/donalexs12/test/runs/gc1omplg' target=\"_blank\">https://wandb.ai/donalexs12/test/runs/gc1omplg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n",
      "global_step=44, episodic_return=[11.], episode_length = [11]\n",
      "global_step=56, episodic_return=[14.], episode_length = [14]\n",
      "global_step=68, episodic_return=[17.], episode_length = [17]\n",
      "global_step=72, episodic_return=[18.], episode_length = [18]\n",
      "global_step=88, episodic_return=[11.], episode_length = [11]\n",
      "global_step=100, episodic_return=[11.], episode_length = [11]\n",
      "global_step=120, episodic_return=[12.], episode_length = [12]\n",
      "global_step=132, episodic_return=[16.], episode_length = [16]\n",
      "global_step=144, episodic_return=[11.], episode_length = [11]\n",
      "global_step=148, episodic_return=[15.], episode_length = [15]\n",
      "global_step=192, episodic_return=[15.], episode_length = [15]\n",
      "global_step=196, episodic_return=[12.], episode_length = [12]\n",
      "global_step=232, episodic_return=[28.], episode_length = [28]\n",
      "global_step=240, episodic_return=[24.], episode_length = [24]\n",
      "global_step=256, episodic_return=[16.], episode_length = [16]\n",
      "global_step=288, episodic_return=[14.], episode_length = [14]\n",
      "global_step=300, episodic_return=[26.], episode_length = [26]\n",
      "global_step=304, episodic_return=[12.], episode_length = [12]\n",
      "global_step=328, episodic_return=[22.], episode_length = [22]\n",
      "global_step=360, episodic_return=[15.], episode_length = [15]\n",
      "global_step=364, episodic_return=[19.], episode_length = [19]\n",
      "global_step=404, episodic_return=[19.], episode_length = [19]\n",
      "global_step=436, episodic_return=[19.], episode_length = [19]\n",
      "global_step=448, episodic_return=[11.], episode_length = [11]\n",
      "global_step=456, episodic_return=[23.], episode_length = [23]\n",
      "global_step=468, episodic_return=[27.], episode_length = [27]\n",
      "global_step=480, episodic_return=[11.], episode_length = [11]\n",
      "global_step=508, episodic_return=[15.], episode_length = [15]\n",
      "global_step=524, episodic_return=[14.], episode_length = [14]\n",
      "global_step=616, episodic_return=[34.], episode_length = [34]\n",
      "global_step=620, episodic_return=[41.], episode_length = [41]\n",
      "global_step=640, episodic_return=[29.], episode_length = [29]\n",
      "global_step=688, episodic_return=[18.], episode_length = [18]\n",
      "global_step=704, episodic_return=[21.], episode_length = [21]\n",
      "global_step=720, episodic_return=[53.], episode_length = [53]\n",
      "global_step=780, episodic_return=[35.], episode_length = [35]\n",
      "global_step=800, episodic_return=[20.], episode_length = [20]\n",
      "global_step=820, episodic_return=[29.], episode_length = [29]\n",
      "global_step=832, episodic_return=[13.], episode_length = [13]\n",
      "global_step=840, episodic_return=[38.], episode_length = [38]\n",
      "global_step=988, episodic_return=[47.], episode_length = [47]\n",
      "global_step=1020, episodic_return=[45.], episode_length = [45]\n",
      "global_step=1040, episodic_return=[55.], episode_length = [55]\n",
      "global_step=1152, episodic_return=[80.], episode_length = [80]\n",
      "global_step=1224, episodic_return=[59.], episode_length = [59]\n",
      "global_step=1368, episodic_return=[82.], episode_length = [82]\n",
      "global_step=1504, episodic_return=[70.], episode_length = [70]\n",
      "global_step=1564, episodic_return=[136.], episode_length = [136]\n",
      "global_step=1700, episodic_return=[83.], episode_length = [83]\n",
      "global_step=1808, episodic_return=[61.], episode_length = [61]\n",
      "global_step=1828, episodic_return=[81.], episode_length = [81]\n",
      "global_step=1916, episodic_return=[191.], episode_length = [191]\n",
      "global_step=1948, episodic_return=[62.], episode_length = [62]\n",
      "global_step=2192, episodic_return=[91.], episode_length = [91]\n",
      "global_step=2216, episodic_return=[67.], episode_length = [67]\n",
      "global_step=2244, episodic_return=[109.], episode_length = [109]\n",
      "global_step=2492, episodic_return=[144.], episode_length = [144]\n",
      "global_step=2528, episodic_return=[84.], episode_length = [84]\n",
      "global_step=2556, episodic_return=[85.], episode_length = [85]\n",
      "global_step=2604, episodic_return=[90.], episode_length = [90]\n",
      "global_step=2672, episodic_return=[17.], episode_length = [17]\n",
      "global_step=2800, episodic_return=[77.], episode_length = [77]\n",
      "global_step=2936, episodic_return=[102.], episode_length = [102]\n",
      "global_step=2944, episodic_return=[68.], episode_length = [68]\n",
      "global_step=3060, episodic_return=[126.], episode_length = [126]\n",
      "global_step=3324, episodic_return=[97.], episode_length = [97]\n",
      "global_step=3396, episodic_return=[84.], episode_length = [84]\n",
      "global_step=3436, episodic_return=[159.], episode_length = [159]\n",
      "global_step=3488, episodic_return=[136.], episode_length = [136]\n",
      "global_step=3652, episodic_return=[82.], episode_length = [82]\n",
      "global_step=3728, episodic_return=[73.], episode_length = [73]\n",
      "global_step=3808, episodic_return=[103.], episode_length = [103]\n",
      "global_step=3832, episodic_return=[86.], episode_length = [86]\n",
      "global_step=4040, episodic_return=[52.], episode_length = [52]\n",
      "global_step=4044, episodic_return=[79.], episode_length = [79]\n",
      "global_step=4128, episodic_return=[80.], episode_length = [80]\n",
      "global_step=4232, episodic_return=[145.], episode_length = [145]\n",
      "global_step=4432, episodic_return=[98.], episode_length = [98]\n",
      "global_step=4452, episodic_return=[81.], episode_length = [81]\n",
      "global_step=4472, episodic_return=[107.], episode_length = [107]\n",
      "global_step=4552, episodic_return=[30.], episode_length = [30]\n",
      "global_step=4672, episodic_return=[110.], episode_length = [110]\n",
      "global_step=4688, episodic_return=[54.], episode_length = [54]\n",
      "global_step=4812, episodic_return=[90.], episode_length = [90]\n",
      "global_step=4872, episodic_return=[46.], episode_length = [46]\n",
      "global_step=4980, episodic_return=[107.], episode_length = [107]\n",
      "global_step=5016, episodic_return=[86.], episode_length = [86]\n",
      "global_step=5148, episodic_return=[84.], episode_length = [84]\n",
      "global_step=5204, episodic_return=[83.], episode_length = [83]\n",
      "global_step=5324, episodic_return=[77.], episode_length = [77]\n",
      "global_step=5356, episodic_return=[94.], episode_length = [94]\n",
      "global_step=5512, episodic_return=[91.], episode_length = [91]\n",
      "global_step=5548, episodic_return=[86.], episode_length = [86]\n",
      "global_step=5612, episodic_return=[64.], episode_length = [64]\n",
      "global_step=5704, episodic_return=[95.], episode_length = [95]\n",
      "global_step=5864, episodic_return=[79.], episode_length = [79]\n",
      "global_step=5964, episodic_return=[113.], episode_length = [113]\n",
      "global_step=6028, episodic_return=[81.], episode_length = [81]\n",
      "global_step=6064, episodic_return=[113.], episode_length = [113]\n",
      "global_step=6268, episodic_return=[76.], episode_length = [76]\n",
      "global_step=6356, episodic_return=[82.], episode_length = [82]\n",
      "global_step=6372, episodic_return=[127.], episode_length = [127]\n",
      "global_step=6620, episodic_return=[139.], episode_length = [139]\n",
      "global_step=6668, episodic_return=[100.], episode_length = [100]\n",
      "global_step=6756, episodic_return=[100.], episode_length = [100]\n",
      "global_step=6972, episodic_return=[150.], episode_length = [150]\n",
      "global_step=7044, episodic_return=[106.], episode_length = [106]\n",
      "global_step=7200, episodic_return=[111.], episode_length = [111]\n",
      "global_step=7252, episodic_return=[146.], episode_length = [146]\n",
      "global_step=7532, episodic_return=[140.], episode_length = [140]\n",
      "global_step=7640, episodic_return=[97.], episode_length = [97]\n",
      "global_step=7648, episodic_return=[151.], episode_length = [151]\n",
      "global_step=7720, episodic_return=[130.], episode_length = [130]\n",
      "global_step=8088, episodic_return=[139.], episode_length = [139]\n",
      "global_step=8208, episodic_return=[142.], episode_length = [142]\n",
      "global_step=8344, episodic_return=[156.], episode_length = [156]\n",
      "global_step=8372, episodic_return=[181.], episode_length = [181]\n",
      "global_step=8632, episodic_return=[106.], episode_length = [106]\n",
      "global_step=8656, episodic_return=[142.], episode_length = [142]\n",
      "global_step=8896, episodic_return=[131.], episode_length = [131]\n",
      "global_step=9076, episodic_return=[183.], episode_length = [183]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>charts/SPS</td><td>▃▃▁▂▂▂▂▂▃▃▄▄▄▄▄▃▃▃▃▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█</td></tr><tr><td>charts/episodic_length</td><td>▁▁▁▁▂▁▁▁▁▁▂▁▁▂▄▃▃▃▅▄▄▆▇▄▃▆▂▄▄▄▄▄▅▆▅▅▅▆██</td></tr><tr><td>charts/episodic_return</td><td>▁▁▁▁▂▁▁▁▁▁▂▁▁▂▄▃▃▃▅▄▄▆▇▄▃▆▂▄▄▄▄▄▅▆▅▅▅▆██</td></tr><tr><td>charts/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>losses/approx_kl</td><td>█▁▂▂▂▁▂▂▄▂▁▁▂▁▁▂▁▂▂▁▁▂▂▁▁▂▁▁▂▂▁▁▂▁▁▁▁▂▁▁</td></tr><tr><td>losses/clipfrac</td><td>█▂▃▄▄▂▅▂▄▃▂▂▄▃▂▂▂▃▃▂▂▃▃▂▂▂▂▂▃▂▂▂▂▂▂▂▁▃▁▁</td></tr><tr><td>losses/entropy</td><td>███▇▇▆▄▅▄▄▄▄▃▃▄▅▆▅▅▄▅▄▅▅▄▅▃▅▄▃▃▃▃▂▁▃▄▃▃▃</td></tr><tr><td>losses/explained_variance</td><td>▂▂▁▃▄▄▄▁▂▂▅▆▅▅▅▆▅▄▄▅▅▄▆▆▇▆█▇▇▇█▇▇▇▇▇▅▇▆█</td></tr><tr><td>losses/old_approx_kl</td><td>█▂▂▂▂▂▁▂▂▂▁▂▁▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▁▂▂▁▂▁▂▂▂</td></tr><tr><td>losses/policy_loss</td><td>▁▆▆▅▄▇▅▇▅▄▇▇▅▆▇▆▆▆▅▇▆▆█▆▇█▆▇▆█▆▅▇▇▇▇▇▇▇▇</td></tr><tr><td>losses/value_loss</td><td>▁▂▂▄▃▃▇▆▅▄▄▄▄▄▄▃▃▄▄▃▃▃▄▄▂▃▁▂▂▂▂▂▂▁▁▂█▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>charts/SPS</td><td>65.0</td></tr><tr><td>charts/episodic_length</td><td>183.0</td></tr><tr><td>charts/episodic_return</td><td>183.0</td></tr><tr><td>charts/learning_rate</td><td>1e-05</td></tr><tr><td>global_step</td><td>9216</td></tr><tr><td>losses/approx_kl</td><td>0.00317</td></tr><tr><td>losses/clipfrac</td><td>0.06913</td></tr><tr><td>losses/entropy</td><td>0.54428</td></tr><tr><td>losses/explained_variance</td><td>0.97032</td></tr><tr><td>losses/old_approx_kl</td><td>-0.00316</td></tr><tr><td>losses/policy_loss</td><td>-0.00638</td></tr><tr><td>losses/value_loss</td><td>16.49305</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">brisk-sweep-1</strong> at: <a href='https://wandb.ai/donalexs12/test/runs/gc1omplg' target=\"_blank\">https://wandb.ai/donalexs12/test/runs/gc1omplg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20231107_235504-gc1omplg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train_PPO, count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa2cbcff-7bf5-4206-96e7-eb04e6e6a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfImitation():\n",
    "    def __init__(self,model_obs,model_values,model_entropy,value,neg_log_prob,action_space\n",
    "                ,reward,obs,n_envs,batch_size,n_updates,clip,w_value,w_entropy,\n",
    "                max_steps,gamma,max_nlogp,min_batch_size,stack,alpha,beta):\n",
    "            self.model_ob = model_obs\n",
    "            self.model_vf = model_values\n",
    "            self.model_entropy = model_entropy\n",
    "            self.fn_value = value\n",
    "            self.fn_neg_log_prob = neg_log_prob\n",
    "            self.fn_reward = reward\n",
    "            self.fn_obs = obs\n",
    "    \n",
    "            self.beta = beta\n",
    "            self.buffer = PrioritizedReplayBuffer(max_steps, alpha)\n",
    "            self.n_env = n_env\n",
    "            self.batch_size = batch_size\n",
    "            self.n_update = n_update\n",
    "            self.clip = clip\n",
    "            self.w_loss = 1.0\n",
    "            self.w_value = w_value\n",
    "            self.w_entropy = w_entropy\n",
    "            self.max_steps = max_steps\n",
    "            self.gamma = gamma\n",
    "            self.max_nlogp = max_nlogp\n",
    "            self.min_batch_size = min_batch_size\n",
    "    \n",
    "            self.stack = stack\n",
    "            self.train_count = 0\n",
    "            self.update_count = 0\n",
    "            self.total_steps = []\n",
    "            self.total_rewards = []\n",
    "            self.running_episodes = [[] for _ in range(n_env)]\n",
    "\n",
    "            self.build_loss_op()\n",
    "\n",
    "        def build_loss_op(self, params, optim, lr, max_grad_norm=0.5):\n",
    "            mask = torch.where(self.R - torch.squeeze(self.model_vf) > 0.0,\n",
    "                               torch.ones_like(self.R),\n",
    "                              torch.zeros_like(self.R))\n",
    "            self.num_valid_samples = torch.sum(mask)\n",
    "            self.num_samples = torch.max(self.num_valid_samples, self.min_batch_size)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c399a392-3f23-40c5-9d20-836f53b84b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640218f1-3993-4be6-8f40-38e3e3324be7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
